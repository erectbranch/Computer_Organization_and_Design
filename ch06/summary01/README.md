# 6 Parallel Processors (Part I)

---

## 6.1 Interconnect

parallel hardware ì„¤ê³„ìë“¤ì€ ëª¨ë“  processorê°€ ê³µìœ í•˜ëŠ” ë‹¨ì¼ physical address spaceë¥¼ ë§Œë“¤ë ¤ê³  í•˜ì˜€ë‹¤. ì´ë¥¼ **shared memory multiprocessor**(SMP)ë¼ê³  í•˜ë©°, multicore chipì€ ê±°ì˜ ì´ ë°©ì‹ìœ¼ë¡œ ì„¤ê³„ëœë‹¤.

![SMP](images/SMP.png)

ë”°ë¼ì„œ CPU, memory, I/O controllers ê°„ì— **interconnection**ì´ í•„ìš”í•˜ê²Œ ë˜ì—ˆë‹¤. ëŒ€í‘œì ì¸ ì˜ˆì‹œë¡œ bus interconnectê°€ ìˆë‹¤.

- **bus**: shared communication channel

  - busì— ì—°ê²°ëœ deviceë“¤ì€ wireë¥¼ ê³µìœ í•˜ê¸° ë•Œë¬¸ì—, íŠ¸ë˜í”½ì´ ë§ì•„ì§€ë©´ bottleneckì´ ë°œìƒí•œë‹¤.

  - wire length, connections ìˆ˜ ë“±ìœ¼ë¡œ performanceê°€ ì œí•œëœë‹¤.

> ë”°ë¼ì„œ ìµœê·¼ì—ëŠ” switchë¥¼ ì´ìš©í•œ high-speed serial connections ê°™ì€ ëŒ€ì•ˆì´ ì¡´ì¬í•œë‹¤.(networkì—ì„œ ì£¼ë¡œ ì‚¬ìš©í•œë‹¤.)

---

### 6.1.1 I/O Register Mapping

CPUê°€ I/Oì— ì ‘ì†í•˜ëŠ” ë°©ë²•ì€ í¬ê²Œ ë‘ ê°€ì§€ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤.

![I/O register mapping](images/io_register_mapping.png)

- **memory mapped I/O**

  - memory registerì™€ I/O resisterê°€ í•˜ë‚˜ì˜ ì—°ì†ëœ memory address ì˜ì—­ì— í• ë‹¹ëœë‹¤.

    > ë”°ë¼ì„œ compilerê°€ ìµœì í™”í•˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´, í”„ë¡œê·¸ë˜ë° ì‹œ I/O ì˜ì—­ì˜ ë³€ìˆ˜ë¥¼ `volatile` íƒ€ì…ìœ¼ë¡œ ì„ ì–¸í•´ì•¼ í•œë‹¤.

  - address decoderê°€ I/O addressë¥¼ êµ¬ë¶„í•œë‹¤.

  - address translation mechanismì„ ì‚¬ìš©í•´ì„œ, ì˜¤ì§ kernel(OS)ì„ í†µí•´ì„œë§Œ ì ‘ê·¼í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤.

    > kernel: memoryì— ìƒì£¼í•˜ëŠ” OSì˜ í•µì‹¬ ë¶€ë¶„. OSëŠ” ì „ì›ì´ ì¼œì§ê³¼ ë™ì‹œì— í•­ìƒ í•„ìš”í•œ ë¶€ë¶„ë§Œ memoryì— ì˜¬ë ¤ì„œ ì‚¬ìš©í•˜ê²Œ ëœë‹¤.(memory ë‚­ë¹„ë¥¼ ë§‰ê¸° ìœ„í•¨)

  - ARM, MIPS ë“±ì´ ëŒ€í‘œì ì´ë‹¤. ì ì€ ë³µì¡ì„± ë•ë¶„ì— ë¡œì§ì´ ê°„ë‹¨í•˜ë©° ì„ë² ë””ë“œ ì‹œìŠ¤í…œì—ì„œ ë§ì´ ì‚¬ìš©ëœë‹¤.

- **I/O mapped I/O**

  - memoryì™€ I/Oê°€ ë³„ê°œì˜ address ì˜ì—­ì— í• ë‹¹ëœë‹¤.

  - Intel x86ì´ ëŒ€í‘œì ì´ë‹¤.

  - ë§ˆì°¬ê°€ì§€ë¡œ kernel modeì—ì„œë§Œ ì ‘ê·¼í•  ìˆ˜ ìˆë‹¤.

---

### 6.1.3 Polling

> [polling, interrupt ì •ë¦¬](https://github.com/erectbranch/Embedded_Architecture/tree/master/ch04)

**polling**ì´ I/O status registerë¥¼ ì£¼ê¸°ì ìœ¼ë¡œ í™•ì¸í•˜ëŠ” ë°©ì‹ì´ë‹¤.

- deviceê°€ ready ìƒíƒœë¼ë©´ `do ()` opertionì„ ì‹œì‘í•œë‹¤. 

- errorì¼ ë•ŒëŠ” actionì„ ì·¨í•œë‹¤.

- ì¥ì ìœ¼ë¡œëŠ” hardware costê°€ ì ì§€ë§Œ, ë‹¨ì ìœ¼ë¡œëŠ” ì¥ì¹˜ registerë¥¼ ê³„ì† ì½ìœ¼ë¯€ë¡œ CPU timeì˜ ë‚­ë¹„ê°€ í¬ë‹¤.

- ì£¼ë¡œ ì‘ê³  ì €ì‚¬ì–‘ì¸ real-time embedded systemì—ì„œ ì‚¬ìš©í•œë‹¤.

---

### 6.1.4 interrupt

> [CPU, OS, task ê°œë… ì •ë¦¬](https://jihooyim1.gitbooks.io/iknowosbasic/content/contents/02.html)

**interrupt**ë€ I/O device ë“±ì—ì„œ ì˜ˆì™¸ê°€ ë°œìƒí•˜ë©´, interrupt ì‚¬ì‹¤ì„ CPUì—ê²Œ ì•Œë ¤ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ë°©ì‹ì´ë‹¤. 

- interruptëŠ” I/O deviceì—ì„œ interrupt controllerë¡œ ì „ë‹¬ë˜ê³ , interrupt controllerëŠ” í•´ë‹¹ interrupt ì¢…ë¥˜ë¥¼ íŒŒì•…í•´ì„œ CPUì—ê²Œ ì „ë‹¬í•œë‹¤. ì´ë¥¼ **IRQ**(Interrupt Request)ë¼ê³  í•œë‹¤.

- OSëŠ” interrupt ì¢…ë¥˜ì— ë§ëŠ” interrupt handlerë¥¼ í˜¸ì¶œ(invoke)í•˜ì—¬ ì²˜ë¦¬í•œë‹¤.

  > OSê°€ í˜„ì¬ CPUê°€ ì‹¤í–‰ ì¤‘ì¸ programì„ ì¤‘ë‹¨ì‹œí‚¤ê³ , ìš°ì„ ìˆœìœ„ê°€ ë†’ì€ taskë¥¼ CPUì—ê²Œ í• ë‹¹í•œë‹¤.

ì°¸ê³ ë¡œ interruptëŠ” software interrupt, hardware interruptë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤.

- software interrupt: DIV ëª…ë ¹ì–´ ì²˜ë¦¬ì—ì„œ 0ìœ¼ë¡œ ë‚˜ëˆ„ê±°ë‚˜, í• ë‹¹ë˜ì§€ ì•Šì€ ê³µê°„ì— accessí•˜ëŠ” ë“±ì˜ ì˜ˆì™¸ ìƒí™©

- hardware interrupt: í‚¤ë³´ë“œ ì…ë ¥, ë””ìŠ¤í¬ ì œì–´ ë“±

---

## 6.2 Heterogeneous Computing

**heterogeneous computing**(ì´ê¸°ì¢… ì»´í“¨íŒ…)ì´ë€ CPUë§Œì´ ì•„ë‹Œ ë‹¤ë¥¸ ì¢…ë¥˜ì˜ coprocessor(**GPU**, FPGA ë“±)ë¥¼ ì¥ì°©í•œ ì‹œìŠ¤í…œì„ ì˜ë¯¸í•œë‹¤.

> ì´ì¤‘ì—ì„œë„ GPUëŠ” parallel processingì— íŠ¹í™”ëœ **manycore processor** êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆë‹¤.

---

### 6.2.1 Parallel programming difficulties

parallel programmingì—ì„œ ë°œëª©ì„ ì¡ëŠ” ì–´ë ¤ìš´ ë¬¸ì œ ì„¸ ê°€ì§€ ì˜ˆì‹œë¥¼ ì‚´í´ë³´ì.

- **partitioning**(=mapping)

    ì‘ì—…ì„ ë³‘ë ¬ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ì¼

- **coordination**(=load-balancing, synchronization)

    ë¶€í•˜ë¥¼ ê· ë“±í•˜ê²Œ í•˜ê³  ë™ê¸°í™”í•˜ëŠ” ì¼

- **communications overhead**(=data movement)

    í†µì‹  overheadë¥¼ ì¤„ì´ëŠ” ì¼

---

### 6.2.2 Parallel programming and Amdahl's law

parallel programmingì—ì„œ speedupì´ ì–¼ë§ˆë‚˜ ì–´ë ¤ìš´ì§€ëŠ” **Amdahl's law**ë¥¼ í†µí•´ ì²´ê°í•  ìˆ˜ ìˆë‹¤.

```math
T_{new} = T_{parallelizable}/100 + T_{sequantial}
```

Amdahl's lawë¥¼ speedupê³¼ ê´€ë ¨ëœ ì‹ìœ¼ë¡œ ë°”ê¾¸ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

```math
speedup = {{Exec \, \, time \, \, before} \over {(Exec \, \, time \, \, before - {Exec \, \, time \, \, affected}) + {Exec \, \, time \, \, affected / Amount \, \, of \, \, improvement}}} 
```

- Exec time before: ì›ë˜ ì‹¤í–‰ì‹œê°„

- Exec time affected: ê°œì„ ë  (ë¶€ë¶„ì˜) ì‹œê°„( $F_{parallelizable}$ )

- Amount of improvement: ê°œì„ ë  ë¶€ë¶„ì˜ ë¹„ìœ¨

ì´ë•Œ Exec time beforeë¥¼ 1ë¡œ ê°€ì •í•˜ë©´, ì‹ì€ ë‹¤ìŒê³¼ ê°™ì´ ë°”ê¿”ì„œ ì“¸ ìˆ˜ ìˆë‹¤.

```math
speedup = {{1} \over {(1 - {Exec \, \, time \, \, affected}) + {Exec \, \, time \, \, affected / Amount \, \, of \, \, improvement}}}
```

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ 1: ì†ë„ ê°œì„ ì˜ ì–´ë ¤ì›€&nbsp;&nbsp;&nbsp;</span>

processor ìˆ˜ë¥¼ 100 processorsë¡œ í•˜ì—¬ 90ë°° speedupì„ ë‹¬ì„±í•˜ê³  ì‹¶ë‹¤ê³  í•˜ì. ì›ë˜ programì—ì„œ ìµœëŒ€ ëª‡ %ê¹Œì§€ sequantial partê°€ ìˆì–´ë„ ë˜ëŠ”ì§€ êµ¬í•˜ë¼.

### <span style='background-color: #C2B2B2; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ” í’€ì´&nbsp;&nbsp;&nbsp;</span>

```math
Speedup = 90 = {{1} \over {(1 - F_{parallelizable}) + {F_{parallelizable}/100}}}
```

ê³„ì‚° ì‹œ $F_{parallelizable}=0.999$ ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ sequential partëŠ” 0.1% ì´í•˜ë¡œ ë‚¨ì•„ì•¼ í•œë‹¤.

---

## 6.3 Process vs Thread

> [process ì •ë¦¬](http://mm.sookmyung.ac.kr/~bigrain/class/2011/mmso/StallingsOS6e-Chap03.pdf), [thread ì •ë¦¬](http://mm.sookmyung.ac.kr/~bigrain/class/2011/mmso/StallingsOS6e-Chap04.pdf)

**process**ì™€ **thread**ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì°¨ì´ì ì´ ìˆë‹¤.

- **process**

  ì‹¤í–‰ ì¤‘ì¸ í”„ë¡œê·¸ë¨(A program in execution). instructions ì§‘í•©ìœ¼ë¡œ í™œë™ì˜ ë‹¨ìœ„ë‹¤.(a unit of activity)
  
  - processëŠ” ìµœì†Œ í•˜ë‚˜ì˜ threadë¥¼ ê°€ì§€ë©°, multiple threadë¥¼ ê°€ì§ˆ ìˆ˜ ìˆë‹¤.

  - CPU resourceë¥¼ í• ë‹¹ë°›ëŠ”ë‹¤.(CPU time, code/data/stackê³¼ ê°™ì€ memory ë“±)

- **thread**

  process ì•ˆì˜ ì œì–´ íë¦„(control flow) ë‹¨ìœ„ë‹¤.

  - í•œ process ì•ˆì˜ ëª¨ë“  threadëŠ” memory ë° resourceë¥¼ ê³µìœ í•œë‹¤.(kernel ê°œì… ì—†ì´ ì„œë¡œ í†µì‹ )

  - local variableì€ ë…ë¦½ì ìœ¼ë¡œ ì‚¬ìš©í•˜ê³ , global variableì€ ê³µìœ í•œë‹¤.

  - processì— ë¹„í•´ ìƒˆë¡œìš´ threadë¥¼ ìƒì„±í•˜ëŠ” í¸ì´ ì‹œê°„ê³¼ ë¹„ìš©ì´ ì ˆì•½ëœë‹¤.

    > ëŒ€í‘œì ìœ¼ë¡œ interruptì— ì˜í•œ **context switching**ì´ ìˆë‹¤. processì˜ ê²½ìš° cache, data, stack, heap ì´ˆê¸°í™”ë¡œ ì¸í•œ ë¹„ìš©ì´ ë“œëŠ”ë°, threadëŠ” ìì‹ ì˜ stack ì˜ì—­ë§Œ ì´ˆê¸°í™”í•˜ë©´ ë˜ê¸° ë•Œë¬¸ì— ë¹„ìš©ì´ í›¨ì”¬ ì‘ë‹¤.

    > context switching ë•Œ processëŠ” **PCB**(Process Control Block)ì— ì˜í•´ ê´€ë¦¬ë˜ë©°, threadëŠ” 'process ë‚´ **TCB**(Task Control Block)'ì— ì˜í•´ ê´€ë¦¬ëœë‹¤.

ë‹¤ìŒì€ **threading**(ë‹¨ì¼ ìŠ¤ë ˆë”©), **multithreading**(ë‹¤ì¤‘ ìŠ¤ë ˆë”©)ì˜ ì˜ˆì‹œë¥¼ ë‚˜íƒ€ë‚¸ ê·¸ë¦¼ì´ë‹¤.

![threading vs multithreading](images/threading_vs_multithreading.png)

í•œ processì—ì„œ single-thread, multi-threadì˜ ìì›ì€ ë‹¤ìŒê³¼ ê°™ì´ ë‚˜ë‰˜ê²Œ ëœë‹¤.

![threading resource](images/singlethread_multithread_resource.png)

> PC registerë„ threadë§ˆë‹¤ ë…ë¦½ì ìœ¼ë¡œ í• ë‹¹ëœë‹¤.

---

### 6.3.1 Temporal Multi-Threading

ê·¸ë ‡ë‹¤ë©´ Multithreadingì€ ì–´ë–»ê²Œ êµ¬í˜„í•˜ëŠ” ê±¸ê¹Œ? Multithreading ì¤‘ì—ì„œë„ **TMT**(Temporal Multi-threading)ì˜ êµ¬í˜„ì€ í¬ê²Œ ë‘ ê°€ì§€ ë°©ì‹ìœ¼ë¡œ ë‚˜ë‰œë‹¤.

> Multi-Threadingì€ í•œ ëª…ì´ ì—¬ëŸ¬ ì¼ì„ ë©€í‹°íƒœìŠ¤í‚¹í•˜ëŠ” TMT, ì—¬ëŸ¬ ëª…ì´ ì—¬ëŸ¬ ì¼ì„ ìŠ¤ì¼€ì¤„ë§ìœ¼ë¡œ ë¶„ë‹´í•˜ëŠ” SMTë¡œ ë¹„ìœ í•  ìˆ˜ ìˆë‹¤.

![temporal multithreading](images/fine_and_coarse_grained_parallelism.png)

- **Coarse-grained multithreading**

  L2 cache missì™€ ê°™ì€ long stallì´ ë°œìƒí—€ì„ ë•Œë§Œ threadë¥¼ switchí•œë‹¤.

  - í•œ clock cycleì— ì—¬ëŸ¬ threadì—ì„œ instructionì„ issueí•œë‹¤.

  - hardwareê°€ ë‹¨ìˆœí•˜ì—¬ êµ¬í˜„ì´ ì‰½ì§€ë§Œ, data hazardsì™€ ê°™ì€ short stallì˜ hidingì´ ë¶ˆê°€ëŠ¥í•˜ë‹¤.

- **Fine-grained multithreading**

  cycleë§ˆë‹¤ threadsë¥¼ switchí•œë‹¤. 

  > thread switchingì„ ìœ„í•œ hardwareë¥¼ ë‘ì–´ overheadë¥¼ ì¤„ì¸ë‹¤.
  
  - ë§Œì•½ í•œ threadê°€ stallë˜ë©´ ë‹¤ë¥¸ threadë¥¼ ì‹¤í–‰í•œë‹¤.

    > ì˜ˆë¥¼ ë“¤ì–´ thread Aì—ì„œ cache missê°€ ë°œìƒí•˜ë©´, latencyë™ì•ˆ ë‹¤ë¥¸ thread Bë¥¼ ì‹¤í–‰í•œë‹¤. ì´ê²ƒì´ ë°”ë¡œ **context switching**ì„ í†µí•œ **latency hiding**ì´ë‹¤.

  - threadë“¤ì€ interleave instruction execution ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬í•œë‹¤.

    > í•œ clock cycleì— í•œ threadë§Œ instructionì„ issueí•œë‹¤/

---

### 6.3.2 Simultaneous Multi-Threading

**SMT**(Simultaneous Multi-Threading)ì´ë€ ì—¬ëŸ¬ threadë“¤ì´ ë™ì‹œì— ë™ì‘í•˜ëŠ” multi-threading ê¸°ë²•ì´ë‹¤.

> Intelì—ì„œ **Hyper-threading**ì´ë¼ê³  ë¶ˆë¦¬ëŠ” ê¸°ìˆ ì´ë‹¤. SMTì™€ ë¹„êµí•˜ì—¬ TMTë¥¼ fake multi-threadingìœ¼ë¡œ ë¶€ë¥´ê¸°ë„ í•œë‹¤.

- function unit(issue slot)ë§Œ avaliableí•˜ë©´, ë‹¤ë¥¸ thread instructionì„ ë™ì‹œì— ìˆ˜í–‰í•œë‹¤.

- ì—¬ëŸ¬ threadë“¤ì´ ë™ì‹œì— ë™ì‘í•˜ê¸° ë•Œë¬¸ì— hardware resourceê°€ ë” í•„ìš”í•˜ë‹¤.

- **dependency**ë¥¼ scheduling, register renamingì„ í†µí•´ ê´€ë¦¬í•œë‹¤. 

---

### 6.3.3 Multi-Threading Example

4ê°œì˜ thread A, B, C, Dê°€ ìˆë‹¤ê³  í•  ë•Œ, ê° ë°©ë²•ë³„ multi-threadingì„ í†µí•œ instruction ì‹¤í–‰ì„ ì‚´í´ë³´ì.

![multithreading example](images/TMT_SMT_example.png)

- Coarse MT: Thread Aì—ì„œ long stallì´ ë°œìƒí•˜ì thread Bë¡œ switchí–ˆë‹¤.

- Fine MT: cycleë§ˆë‹¤ thread A, B, C, Dë¥¼ switchí–ˆë‹¤.

- SMT: ê°€ëŠ¥í•œ issue slotì—ì„œ instructionì„ ë™ì‹œì— ì‹¤í–‰í–ˆë‹¤.

---

## 6.4 shared memory multiprocessor

parallel hardware systemì€ physical addressë¥¼ ê³µìœ í•˜ëŠ”ê°€ ì•„ë‹Œê°€ì— ë”°ë¼ í¬ê²Œ ë‘ ê°€ì§€ë¡œ ë‚˜ë‰œë‹¤. ì´ ì¤‘ì—ì„œë„ single physical address spaceë¥¼ ê³µìœ í•˜ëŠ” systemì´ ë°”ë¡œ **SMP**(shared memory multiprocessor)ì´ë‹¤.

> lockì„ ì´ìš©í•´ì„œ shared variablesë¥¼ synchronizeí•œë‹¤.

parallel hardware systemì˜ memory access ë°©ì‹ë„ ë‘ ê°€ì§€ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤.

- **UMA**(Uniform Memory Access)

  ëª¨ë“  processorê°€ ì„œë¡œ ì—°ê²°ë˜ì–´ í•˜ë‚˜ì˜ memoryë¥¼ ê³µìœ í•œë‹¤. 
  
  ë‹¨, ë‹¨ì¼ memory controllerë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ bandwidthê°€ ì œì•½ë˜ê³ , bottleneckì´ ë°œìƒí•  ìˆ˜ ìˆë‹¤.

- **NUMA**(Non-Uniform Memory Access)

  ê° processorê°€ ìì‹ ì˜ local memoryë¥¼ ê°–ëŠ”ë‹¤. 
  
  ìì²´ local memoryì—ì„œ memory access timeì€ ë¹ ë¥´ì§€ë§Œ, ë‹¤ë¥¸ processorì˜ memoryì— ì ‘ê·¼í•  ë•ŒëŠ” ì„±ëŠ¥ì´ ì €í•˜ëœë‹¤.

---

### 6.4.1 programming for SMP

SMPë¥¼ ìœ„í•œ **Thread Programming**ì„ ì•Œì•„ë³´ì. ìš°ì„  thread programmingì„ ì ìš©í•˜ì§€ ì•Šì€ callback functionì˜ íë¦„ê³¼ thread programmingì„ ë¹„êµí•´ ë³´ì.

- callback function

  instruction streamì´ callerì—ì„œ calleeë¡œ ì „í™˜ëë‹¤ê°€, ì‘ì—…ì´ ëë‚˜ë©´ callerë¡œ ëŒì•„ì˜¤ëŠ” íë¦„ì„ ê°–ëŠ”ë‹¤.(ì¦‰, ë™ì‹œì— ì‹¤í–‰ë˜ì§€ ì•ŠëŠ”ë‹¤.)

    ![procedure call](images/procedure_call.png)

- thread programming

  thread1(master)ì—ì„œ thread2(slave)ê°€ ì‹¤í–‰ë˜ëŠ” ë™ì‹œì—, thread1ë„ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆë‹¤.

    ![TLP](images/TLP_concept.png)

---

### 6.4.2 Pthread Programming

**Pthread**(=POSIX thread)ëŠ” thread programmingì„ ìœ„í•œ API(Application Programming Interface)ì´ë‹¤. 

> POSIXëŠ” IEEE standardë¡œ, UNIX, Linux, Solaris, Mac OS X, MS Windosì—ì„œ ì‚¬ìš©í•œë‹¤.

> Cì–¸ì–´ì˜ ê²½ìš° ê´€ë ¨ í—¤ë” íŒŒì¼ì´ `/usr/include/` ë””ë ‰í„°ë¦¬ì— ìˆëŠ” `pthread.h`ì— ì •ì˜ë˜ì–´ ìˆë‹¤. thread libraryë¥¼ ì‚¬ìš©í•˜ëŠ” ê° source fileì— í¬í•¨ë˜ì–´ì•¼ í•œë‹¤.

ë‹¤ìŒì€ A, B vectorë¥¼ ë”í•˜ëŠ” vector addition Pthread Exampleì´ë‹¤.

- `pthread_t threads[4]`: threadë¥¼ 4ê°œ ìƒì„±í•œë‹¤.

- `pthread_create(&threads[i], NULL, TaskCode, (void *)&args[i])`: threadë¥¼ ì •ì˜í•œë‹¤.

    - `&threads[i]`: thread idë¥¼ ì €ì¥í•  address

    - `NULL`: thread attribute (default: NULL)

    - `TaskCode`: threadê°€ ì‹¤í–‰í•  function

    - `(void *)&args[i]`: function ì¸ì

- `pthread_join`: thread completeë¥¼ ê¸°ë‹¤ë¦°ë‹¤.

    - `threads[i]`: ì¢…ë£Œë  thread id

    - `NULL`: return value (default: NULL)

```c
#include <pthread.h>
#include <stdio.h>
#include <stdlib.h>

extern int S[100], A[100], B[100];

void *TaskCode(void *argument)
{
    int tid;

    tid = *((int *) argument);               // pointer cast

    for(int i=tid*25; i<(tid+1)*25; i++){    // loop unrolling
        S[i] = A[i] + B[i];
    }

    return NULL;
}

int main (int argc, char *argv[])
{
    pthread_t threads[4];
    int args[4];
    int i;

    for (i = 0; i < 4; ++i){
        args[i] = i;
        pthread_create(&threads[i], NULL, TaskCode, (void *)&args[i]);
    }

    /* wait for all threads to complete */
    for (i = 0; i < 4; ++i){
        pthread_join(threads[i], NULL);
    }

    exit(EXIT_SUCCESS);
}
```

---

## 6.5 shared memory multiprocessor programming example

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ 2: sum reduction&nbsp;&nbsp;&nbsp;</span>

100 processor UMAì—ì„œ 100,000ê°œ ìˆ«ìë¥¼ ë”í•œë‹¤ê³  í•˜ì. Sum Reductionì„ êµ¬í˜„í•˜ë¼.

![sum reduction](images/sum_reduction.png)

> reduction: divide and conquer. ê° reduction stepì—ì„œëŠ” synchronizeê°€ í•„ìš”í•˜ë‹¤.

- ê° processorëŠ” IDë¥¼ 0~99 ë²”ìœ„ ë‚´ì—ì„œ ê°–ëŠ”ë‹¤.(`myid`)

- processorë§ˆë‹¤ ìˆ«ì 1,000ê°œë¥¼ ê³„ì‚°í•˜ë„ë¡ partitioningí•œë‹¤.

### <span style='background-color: #C2B2B2; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ” í’€ì´&nbsp;&nbsp;&nbsp;</span>

- step1: ìš°ì„  ê° processorê°€ ìˆ«ì 1,000ê°œì—ì„œ ì›ì†Œ 2ê°œì”©ì„ ë”í•œë‹¤.

```c
sum[myid] = 0;
for (i = 1000*myid; i < 1000*(myid+1); i++){
    sum[myid] = sum[myid] + totalData[i];
}
```

- step2: partial sumsì„ êµ¬í˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

```c
for (int stride=100/2; stride > 0; stride /= 2) {
    if (myid < stride) {
        sum[myid] = sum[myid + stride]
    }
    synch();     // ë‹¤ë¥¸ thread ì—°ì‚°ì´ ëë‚  ë•Œ ê¹Œì§€ barrier
}
```

---

## 6.6 Message Passing

processëŠ” (1) ë‹¤ë¥¸ processorì™€ í˜‘ë ¥í•˜ì§€ ì•ŠëŠ” process, (2) ì˜í–¥ì„ ì£¼ê³  ë°›ìœ¼ë©° resourceë¥¼ ê³µìœ í•˜ëŠ” process(**cooperating process**)ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤. ê·¸ëŸ°ë° ê° processorëŠ” private physical address spaceë¥¼ ê°–ê¸° ë•Œë¬¸ì—, processor ì‚¬ì´ì—ì„œ ê³µí†µëœ ê·œì¹™ì˜ hardware send/receives messageê°€ í•„ìš”í•˜ë‹¤.

> ë¹„ìŠ·í•˜ê²Œ networkë¡œ ì—°ê²°ëœ ë‹¤ë¥¸ computerë„ ê°ìì˜ private memoryê³¼ OSë¥¼ ê°–ëŠ”ë‹¤. Ethernet/switch, Internetê³¼ ê°™ì€ I/O systemì„ í†µí•´ ì—°ê²°ë˜ëŠ”ë°, ì´ë•Œë„ message passingì´ í•„ìš”í•˜ë‹¤.

- **IPC**(Inter-Process Communication): cooperating process ì‚¬ì´ì—ì„œ ì„œë¡œ dataë¥¼ ì£¼ê³  ë°›ëŠ” í–‰ìœ„, ë˜ëŠ” ê·¸ ë°©ë²•ê³¼ ê²½ë¡œ

ì´ëŸ¬í•œ clustersë¥¼ êµ¬ì„±í•˜ë©´ì„œ ì–»ëŠ” ì¥ë‹¨ì ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

- ì¥ì 

    - high availability(ì–´ë–¤ ì»´í“¨í„°ê°€ ê³ ì¥ì´ ë‚˜ë©´, ë‹¤ë¥¸ ì»´í“¨í„°ì—ì„œ ì¦‰ì‹œ ì—…ë¬´ë¥¼ ëŒ€ì‹ í•  ìˆ˜ ìˆë‹¤.)
    
    - scalability
    
    - cost-effective

- ë‹¨ì 

   - administration cost

   - low interconnect bandwidth

---

### 6.6.1 Message Passing Interface

ë”°ë¼ì„œ distributed memory systemsì—ì„œ parallel programmingì„ ìœ„í•œ communication protocolë¡œ **MPI**(Message Passing Interface)ë¥¼ ì‚¬ìš©í•œë‹¤.

> ì£¼ë¡œ HPC(High-Performance Computing)ì—ì„œ ì‚¬ìš©í•œë‹¤. `MPI_Send()`, `MPI_Recv()`, `MPI_Broadcast()`...

ë‹¤ìŒì€ MPI Programmingì„ ì´ìš©í•œ vector addition ì˜ˆì‹œë‹¤.

```c
#include <mpi.h>
#include <stdio.h>
#include <stdlib.h>
#define TAG 0

int main(int argc, char *argv[])
{
    int buff[25], numprocs, myid;
    int i;
    MPI_Status stat;

    MPI_Init(&argc, &argv);
    MPI_Comm_size(MPI_COMM_WORLD, &numprocs); // í˜„ì¬ ì˜ˆì œì—ì„œ numprocs = 4ë¡œ ê°€ì •í•œë‹¤.
    MPI_Comm_rank(MPI_COMM_WORLD, &myid);     // processor idë¥¼ ì €ì¥í•œë‹¤.

    // id 0 processor = master processor
    if (myid == 0) {
        int S[100], A[100], B[100];    // A, BëŠ” ì´ë¯¸ ì´ˆê¸°í™”ë˜ì–´ ìˆë‹¤ê³  ê°€ì •

        for (i = 1, i<numprocs; i++) {
            MPI_Send(A, 25, MPI_INT, i, TAG, MPI_COMM_WORLD);
            MPI_Send(B, 25, MPI_INT, i, TAG, MPI_COMM_WORLD);
        }

        int offset = 0;
        for (i = 1; i < numprocs; i++) {
            MPI_Recv(&S[offset], 25, MPI_INT, i, TAG, MPI_COMM_WORLD, &stat);
            offset += 25;
        }
    }
    else {
        int Sum[25];
        MPI_Recv(A, 25, MPI_INT, 0, TAG, MPI_COMM_WORLD, &stat);
        MPI_Recv(B, 25, MPI_INT, 0, TAG, MPI_COMM_WORLD, &stat);

        for (i = 0; i < 25; i++) {
            Sum[i] = A[i] + B[i];
        }

        MPI_Send(S, 25, MPI_INT, 0, TAG, MPI_COMM_WORLD);
    }

    MPI_Finalize();
    return 0;
}

```

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ 3: sum reduction + MPI&nbsp;&nbsp;&nbsp;</span>

100 processor UMAì—ì„œ 100,000ê°œ ìˆ«ìë¥¼ ë”í•˜ëŠ” ì˜ˆì œ 2ë²ˆ sum reductionì„ MPIë¥¼ ì‚¬ìš©í•˜ì—¬ êµ¬í˜„í•˜ë¼.

### <span style='background-color: #C2B2B2; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ” í’€ì´&nbsp;&nbsp;&nbsp;</span>

- step 1: ê° processorì—ì„œ ìˆ˜í–‰í•˜ëŠ” partial sums

```c
sum = 0;
for (i = 0; i < 1000; i++){
    sum = sum + myData[i]
}
```

- step 2: reduction

  processors ì ˆë°˜ì€ send, ë‚˜ë¨¸ì§€ ì ˆë°˜ì€ receice, addë¥¼ ìˆ˜í–‰í•œë‹¤.

  > send, receiveëŠ” ë™ì‹œì— synchronizationë„ ìˆ˜í–‰í•œë‹¤.

```c
limit = 100; half = 100;               // 100ê°œ processor
do {
    half = (half + 1) / 2;             // sendì™€ receive ë¶„ë°°
    
    // send
    if (myid >= half && myid < limit){ 
        send(myid - half, sum);
    }
    // receive, sum
    if (myid < (limit/2)){
        sum = sum + receive();
    }
    limit = half;
} while (half == 1);                   // final sum
```

---